---
title: 04 神经网络和反向传播
attachments: [Clipboard_2025-05-16-13-44-36.png, Clipboard_2025-05-16-13-46-33.png, Clipboard_2025-05-16-14-28-41.png]
tags: [deepLearning]
created: 2025-05-16T05:37:12.938Z
modified: 2025-05-16T06:33:51.126Z
---

# 04 神经网络和反向传播
## 多层感知机
### 单层感知机->多层感知机
![](../dpL/M1.png)
![](../dpL/M2.png)

### 常见激活函数
![](../dpL/M3.png)

性质三指的是：**非零中心化的输出会使得其后一层的神经元的输入产生偏置偏移 (bias shift)，并进一步使得梯度下降的收敛速度变慢。**

简单来说，如果一个激活函数的输出不是以0为中心的（比如 Sigmoid 函数的输出范围是 0 到 1，均值大于0），那么它作为下一层神经元的输入时，这些输入值都会是正数。

这种非零中心化的输入会导致以下问题：

1.  **偏置偏移 (Bias Shift)**: 想象一下，**如果输入到某个神经元的值总是正的，那么在反向传播计算梯度时，对这个神经元权重进行的更新可能都会是同方向的（要么都增加，要么都减少）。这会使得权重更新的路径呈现 "Z" 字形**，而不是直接朝向最优解，从而降低了学习效率。

2.  **收敛速度变慢**: 由于上述的偏置偏移问题，梯度下降算法在寻找最优解的过程中会花费更多的时间，导致模型的整体收敛速度减慢。

相比之下，像 Tanh 这样的激活函数，其输出是以0为中心的（范围是 -1 到 1），可以缓解这个问题，从而可能带来更快的收敛速度。这就是为什么在实践中，当需要非线性激活时，Tanh 有时比 Sigmoid 更受青睐的原因之一。

---
![](../dpL/M4.png)

### 激活函数的必要性
![](../dpL/M6.png)

## 前馈神经网络：信号从输入->输出，单向传播
### 人工神经网络
![](../dpL/M5.png)

![](../dpL/M7.png)

### 前馈全连接神经网络
![](../dpL/M8.png)

### 逻辑回归模型与神经网络模型
![](../dpL/M9.png)

![](../dpL/M10.png)

![](../dpL/M11.png)

### 通用近似定理
![](../dpL/M12.png)

## 梯度反向传播
### 参数学习
![](../dpL/M13.png)

### 梯度下降：当前值-步长*方向
![](../dpL/M14.png)
![](../dpL/M15.png)
![](../dpL/M17.png)

#### 反向传播算法

![](../dpL/M16.png)
![](../dpL/M24.png)
![](../dpL/M25.png)

#### 自动微分：记录每一步的求导结果
![](../dpL/M26.png)
![](../dpL/M27.png)
![](../dpL/M28.png)

##### 静态动态计算图以及优缺点
![](../dpL/M29.png)

---
**梯度下降是一种优化算法，而反向传播是一种计算梯度的方法，特别用于训练神经网络。**

让我们分开理解：

1.  **梯度下降 (Gradient Descent)**

	* **目标：** 找到一个函数的最小值。在机器学习和深度学习中，这个函数通常是**损失函数（Loss Function）**或**成本函数（Cost Function）**。损失函数衡量了模型预测结果与实际结果之间的差异；我们希望通过调整模型的参数（如神经网络中的权重和偏置），使损失函数的值最小化。
	* **如何工作：** 梯度下降通过迭代地调整参数来逼近最小值。它利用函数的**梯度（Gradient）**信息。梯度是一个向量，指向函数值增加最快的方向。要找到最小值，我们需要朝着梯度**相反**的方向移动，因为梯度的反方向是函数值下降最快的方向。
	* **更新规则：**
			$参数_{新} = 参数_{旧} - 学习率 \times 梯度$
			其中，“学习率”（Learning Rate）是一个小的正数，控制着每一步调整参数的步长大小。
	* **理解：** 想象你在一个山上（损失函数是一个曲面），你想走到山谷（最小值）。梯度下降就像是一个盲人下山，他只能感受到脚下坡度的方向（梯度）。为了尽快下山，他会选择坡度最陡的方向往下走一步（沿着梯度的反方向）。然后重复这个过程，直到走到山谷底部。

2.  **反向传播 (Backpropagation)**

	* **目标：** 在神经网络中，高效地计算损失函数关于**所有**权重（Weights）和偏置（Biases）的梯度。这些梯度正是梯度下降算法进行参数更新所需要的。
	* **如何工作：** 神经网络通过一系列的计算（矩阵乘法、激活函数等）将输入转化为输出。这个过程称为**前向传播（Forward Pass）**。在前向传播完成后，我们可以计算出当前的损失值。反向传播利用微积分中的**链式法则（Chain Rule）**，从网络的输出层开始，一层一层地向后（向输入层）计算每个参数对总损失的贡献，即计算损失函数关于每个参数的梯度。
			* 从输出层的损失开始，计算损失对输出层神经元输入的梯度。
			* 使用链式法则，将这个梯度“传播”到前一层，计算损失对该层神经元输入的梯度。
			* 同时，利用链式法则，计算损失对该层权重和偏置的梯度。
			* 重复这个过程，直到计算出损失对所有层的权重和偏置的梯度。
	* **为什么需要它？** 如果没有反向传播，对于包含数百万甚至数十亿参数的深度神经网络，单独计算每个参数的梯度将是极其耗时和计算量巨大的。反向传播提供了一种系统且高效的方式，在一次计算过程中就能得到所有参数的梯度。

3.  **关系：反向传播服务于梯度下降**

	* 梯度下降是**优化策略**：它定义了如何利用梯度来更新参数以最小化损失。
	* 反向传播是**计算方法**：它提供了梯度下降所需要的、在神经网络中计算损失函数关于参数的梯度的方法。
	* 可以这样理解：梯度下降是那个想要下山的人，他知道要沿着坡度最陡的方向往下走。反向传播则是那个能够**告诉**他当前脚下哪个方向最陡峭的工具或方法。每走一步（梯度下降的一次迭代），都需要反向传播来计算出下一步该往哪个方向走（计算出当前的梯度）。

**总结：**

在训练一个神经网络时，整个过程通常是：

1.  进行**前向传播**，将输入数据通过网络计算得到输出，并计算当前的损失值。
2.  进行**反向传播**，利用链式法则高效地计算损失函数关于网络中所有权重和偏置的梯度。
3.  利用**梯度下降**（或其变体，如随机梯度下降SGD、Adam等）算法，根据反向传播计算出的梯度和设定的学习率，更新网络的权重和偏置。
4.  重复步骤1-3，直到损失函数收敛到最小值（或者达到预设的训练轮数）。

因此，反向传播是实现梯度下降训练神经网络的**核心计算引擎**。梯度下降是更高层面的优化目标和策略，而反向传播是为实现这个策略提供必要信息（梯度）的具体算法。


### QA

理解“贡献度”在这里指的是一个参数**对损失函数值变化的影响程度**，而不是说这个参数本身“产生”了多少损失。

在训练神经网络时，我们的目标是最小化损失函数 $L$。我们通过调整网络的参数（权重 $w$ 和偏置 $b$）来实现这个目标。

梯度（或偏导数）正是衡量一个函数值如何随着其输入的变化而变化的工具。具体来说，$\frac{\partial L}{\partial w_i}$ 表示当权重 $w_i$ 发生一个微小的变化时，损失函数 $L$ 将会如何变化（变化的速率和方向）。

为什么说这个梯度 $\frac{\partial L}{\partial w_i}$ 代表了 $w_i$ 的“贡献度”呢？

1.  **指示变化方向和大小：**
	* 梯度的**符号**指示了增加参数 $w_i$ 会导致损失函数增加还是减少。
			* 如果 $\frac{\partial L}{\partial w_i} > 0$，增加 $w_i$ 会增加 $L$。
			* 如果 $\frac{\partial L}{\partial w_i} < 0$，增加 $w_i$ 会减少 $L$。
	* 梯度的**绝对值** $|\frac{\partial L}{\partial w_i}|$ 指示了损失函数对参数 $w_i$ 变化的**敏感性**。
			* 绝对值越大，意味着损失函数对 $w_i$ 的变化越敏感，微小的 $w_i$ 变化会导致更大的 $L$ 变化。
			* 绝对值越小，意味着损失函数对 $w_i$ 的变化不那么敏感，微小的 $w_i$ 变化只导致较小的 $L$ 变化。

2.  **指导参数更新：** 在梯度下降中，我们根据这个梯度来更新参数：
	$w_{i,新} = w_{i,旧} - 学习率 \times \frac{\partial L}{\partial w_i}$
	* 梯度的符号决定了我们应该增加还是减少 $w_i$ 来减小 $L$（沿着梯度的反方向）。
	* 梯度的绝对值决定了我们应该以多大的“力度”来更新 $w_i$。对于一个对损失影响很大的参数（梯度绝对值大），我们会更新得更多一些；对于一个影响较小的参数（梯度绝对值小），我们会更新得少一些。

所以，“贡献度”在这里更准确地理解为参数**对损失函数值当前变化趋势的“影响力”或“敏感度”**。梯度越大（无论是正还是负，看绝对值），说明这个参数对当前损失函数的**变化率**影响越大，调整它能更有效地改变损失函数的值。

反向传播算法正是提供了这种高效计算这些“影响力”/“敏感度”的方式，让梯度下降知道在当前状态下，每个参数应该朝着哪个方向、以多大的相对步长进行调整，以便最快地降低总体的损失。

### 梯度下降-计算图
![](../dpL/M18.png)
![](../dpL/M.19.png)
![](../dpL/M20.png)
![](../dpL/M21.png)
**单个样本**
![](../dpL/M22.png)

**m个样本**
![](../dpL/M23.png)

## 图像分类流程
### 训练流程
![](../dpL/M30.png)



